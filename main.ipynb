{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1051c1a3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c735fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043840f",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df4f00",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caecaece",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 6 \n",
    "LEARNING_RATE = 0.0001\n",
    "IMG_SIZE = 32\n",
    "N_REPEATS = 1 # Number of times to repeat the training for averaging results\n",
    "NUM_CLASSES = 10 # CIFAR-10 dataset\n",
    "# NUM_CLASSES = 100 # CIFAR-100 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a625ac",
   "metadata": {},
   "source": [
    "## Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97631294",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('saved_models', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('histories', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16c44d",
   "metadata": {},
   "source": [
    "# Utilities functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113248cc",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f79125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "def get_cifar10_data():\n",
    "    \"\"\"Load and normalize the CIFAR-10 dataset.\"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def get_cifar100_data():\n",
    "    \"\"\"Load and normalize the CIFAR-100 dataset.\"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data(label_mode='fine')\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def rgb_to_grayscale(images):\n",
    "    \"\"\"Convert RGB images to grayscale using standard weights.\"\"\"\n",
    "    return np.expand_dims(\n",
    "        np.dot(images[...,:3], [0.299, 0.587, 0.114]), axis=-1\n",
    "    )\n",
    "\n",
    "def grayscale_to_rgb(images):\n",
    "    \"\"\"Convert single-channel grayscale images to 3-channel by copying.\"\"\"\n",
    "    return np.concatenate([images]*3, axis=-1)\n",
    "\n",
    "def data_augmentation(img):\n",
    "    \"\"\"Apply simple augmentations to an image tensor.\"\"\"\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_contrast(img, 0.85, 1.15)\n",
    "    img = tf.image.random_brightness(img, 0.1)\n",
    "    img = tf.image.rot90(img, k=np.random.randint(0, 4))\n",
    "    img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "    return img\n",
    "\n",
    "def make_dataset(x, y, is_training=True, augment=False, grayscale=False):\n",
    "    \"\"\"\n",
    "    Create a TensorFlow dataset for training or testing.\n",
    "    Optionally convert to grayscale and/or apply augmentation.\n",
    "    \"\"\"\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    def preprocess(img, label):\n",
    "        if grayscale:\n",
    "            img = tf.image.rgb_to_grayscale(img)\n",
    "        if augment:\n",
    "            img = data_augmentation(img)\n",
    "        return img, label\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    if is_training:\n",
    "        ds = ds.shuffle(buffer_size=1024)\n",
    "    ds = ds.map(lambda img, lbl: preprocess(img, lbl), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def random_grayscale_augmentation(img):\n",
    "    \"\"\"\n",
    "    Augmentation function to apply random flip, contrast, brightness, and rotation to an image.\n",
    "    Used for offline data augmentation.\n",
    "    \"\"\"\n",
    "    img = tf.convert_to_tensor(img)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "    img = tf.image.random_brightness(img, 0.1)\n",
    "    img = tf.image.rot90(img, k=np.random.randint(0, 4))\n",
    "    img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "    return img.numpy()\n",
    "\n",
    "def generate_offline_augmented_set(x, y, augment_fn, seed=42):\n",
    "    \"\"\"\n",
    "    Augment the entire training set offline (ahead of time) and return the combined set.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N = x.shape[0]\n",
    "    augmented = []\n",
    "    for i in range(N):\n",
    "        img = x[i]\n",
    "        img_aug = augment_fn(img)\n",
    "        augmented.append(img_aug)\n",
    "    x_aug = np.stack(augmented)\n",
    "    y_aug = np.copy(y)\n",
    "    # Combine original and augmented data\n",
    "    x_total = np.concatenate([x, x_aug], axis=0)\n",
    "    y_total = np.concatenate([y, y_aug], axis=0)\n",
    "    return x_total, y_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3314103c",
   "metadata": {},
   "source": [
    "## Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8783499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpu_memory():\n",
    "    \"\"\"Return the current process memory usage in MB.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_mb = process.memory_info().rss / 1024**2\n",
    "    return mem_mb\n",
    "\n",
    "def measure_inference_time(model, ds, num_batches=20):\n",
    "    \"\"\"Measure average inference time and memory on the CPU for a few batches.\"\"\"\n",
    "    times = []\n",
    "    mems = []\n",
    "    it = iter(ds)\n",
    "    for _ in range(num_batches):\n",
    "        try:\n",
    "            x_batch, _ = next(it)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        start = time.time()\n",
    "        _ = model.predict(x_batch, verbose=0)\n",
    "        t = time.time() - start\n",
    "        m = get_cpu_memory()\n",
    "        times.append(t)\n",
    "        mems.append(m)\n",
    "    return np.mean(times), np.mean(mems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9136caf9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e997cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(input_shape, num_classes=10):\n",
    "    \"\"\"Build a simple CNN model for CIFAR-10 or CIFAR-100.\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e94810",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    name, x_train, y_train, x_test, y_test, \n",
    "    grayscale=False, augment=False, repeat=0, save_hist=True, seed=None, \n",
    "    offline_augmented=False):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model for a specific experiment configuration.\n",
    "    Saves results and learning curves to disk.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Experiment: {name} | Grayscale: {grayscale} | Augment: {augment} | Repeat: {repeat} ===\")\n",
    "    \n",
    "    # Decide input shape: grayscale images have 1 channel, color have 3\n",
    "    input_shape = (IMG_SIZE, IMG_SIZE, 1) if grayscale or offline_augmented else (IMG_SIZE, IMG_SIZE, 3)\n",
    "    \n",
    "    # Only apply grayscale conversion if images are not already grayscale\n",
    "    train_grayscale = grayscale and not offline_augmented\n",
    "    test_grayscale = grayscale and not offline_augmented\n",
    "    \n",
    "    # Make datasets\n",
    "    ds_train = make_dataset(x_train, y_train, is_training=True, augment=(augment and not offline_augmented), grayscale=train_grayscale)\n",
    "    ds_test = make_dataset(x_test, y_test, is_training=False, augment=False, grayscale=test_grayscale)\n",
    "    \n",
    "    # Build and compile model\n",
    "    model = build_cnn(input_shape, num_classes=NUM_CLASSES)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    if seed is not None:\n",
    "        set_seeds(seed)\n",
    "    \n",
    "    # Train model\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        patience=3, \n",
    "        restore_best_weights=True, \n",
    "        monitor='val_loss'\n",
    "    )\n",
    "    \n",
    "    start_train_time = time.time()\n",
    "    history = model.fit(ds_train, epochs=EPOCHS, validation_data=ds_test, verbose=2, callbacks=[early_stopping])\n",
    "    train_time = time.time() - start_train_time\n",
    "    \n",
    "    # Measure memory and inference\n",
    "    cpu_mem_train = get_cpu_memory()\n",
    "    val_loss, val_acc = model.evaluate(ds_test, verbose=0)\n",
    "    inf_time_cpu, inf_mem_cpu = measure_inference_time(model, ds_test)\n",
    "    \n",
    "    model_name = f\"{name}_rep{repeat}\"\n",
    "    model.save(f\"saved_models/{model_name}.keras\")\n",
    "    \n",
    "    results = {\n",
    "        \"experiment\": name,\n",
    "        \"repeat\": repeat,\n",
    "        \"grayscale\": grayscale,\n",
    "        \"augment\": augment,\n",
    "        \"offline_augmented\": offline_augmented,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"val_acc\": float(val_acc),\n",
    "        \"val_loss\": float(val_loss),\n",
    "        \"train_time_sec\": float(train_time),\n",
    "        \"cpu_mem_train_MB\": float(cpu_mem_train),\n",
    "        \"inf_time_cpu_sec\": float(inf_time_cpu),\n",
    "        \"inf_mem_cpu_MB\": float(inf_mem_cpu),\n",
    "        \"model_params\": model.count_params(),\n",
    "    }\n",
    "    with open(f\"results/{model_name}.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    if save_hist:\n",
    "        with open(f\"histories/{model_name}_history.json\", \"w\") as f:\n",
    "            json.dump(history.history, f, indent=2)\n",
    "    print(\"Results:\", results)\n",
    "    return results, history.history     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31f75a",
   "metadata": {},
   "source": [
    "**Cell to run all experiments:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef062e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = get_cifar100_data()\n",
    "# Precompute grayscale versions for efficiency\n",
    "x_test_gray = rgb_to_grayscale(x_test).astype(\"float32\")\n",
    "x_test_gray_as_rgb = grayscale_to_rgb(x_test_gray)\n",
    "x_train_gray = rgb_to_grayscale(x_train).astype(\"float32\")\n",
    "\n",
    "experiments = [\n",
    "    # Baseline: Color images\n",
    "    dict(name=\"color_baseline\", x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, grayscale=False, augment=False),\n",
    "    # Baseline: Grayscale images (no augmentation)\n",
    "    dict(name=\"grayscale_baseline\", x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, grayscale=True, augment=False),\n",
    "    # Grayscale images with augmentation\n",
    "    dict(name=\"grayscale_aug\", x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, grayscale=True, augment=True),\n",
    "    # Extra controls: Color model on grayscale test images\n",
    "    dict(name=\"color_on_gray_test\", x_train=x_train, y_train=y_train, x_test=x_test_gray_as_rgb, y_test=y_test, grayscale=False, augment=False),\n",
    "    dict(name=\"grayscale_on_color_test\", x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, grayscale=True, augment=False),\n",
    "    dict(name=\"grayscale_aug_on_color_test\", x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, grayscale=True, augment=True)\n",
    "]\n",
    "\n",
    "# Offline-augmented grayscale experiment (doubles training data size)\n",
    "print(\"Generating offline-augmented grayscale training set...\")\n",
    "for rep in range(N_REPEATS):\n",
    "    seed = 42 + rep\n",
    "    x_train_gray_offline, y_train_gray_offline = generate_offline_augmented_set(\n",
    "        x_train_gray, y_train, random_grayscale_augmentation, seed=seed\n",
    "    )\n",
    "    run_experiment(\n",
    "        name=\"grayscale_offline_aug_100k\",\n",
    "        x_train=x_train_gray_offline, y_train=y_train_gray_offline,\n",
    "        x_test=x_test_gray, y_test=y_test,\n",
    "        grayscale=False, augment=False, repeat=rep, seed=seed,\n",
    "        offline_augmented=True\n",
    "    )\n",
    "\n",
    "# Run all other experiments\n",
    "for exp in experiments:\n",
    "    for rep in range(N_REPEATS):\n",
    "        seed = 42 + rep\n",
    "        run_experiment(repeat=rep, seed=seed, **exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
